{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import requests\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import DataFrame as Sdf2\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(1,2):\n",
    "    for i in range (0,24):\n",
    "        response=requests.get('https://data.gharchive.org/2015-01-0'+str(j)+'-'+str(i)+'.json.gz',stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open('dataPS.json.gz', 'ab') as f:\n",
    "                f.write(response.raw.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark =SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"FlatJson\") \\\n",
    ".master(\"local[*]\").getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "\n",
    "df=spark.read.json('dataPS.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flattening Json Data Dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(df2):\n",
    "   # compute Complex Fields (Lists and Structs) in Schema   \n",
    "   complex_fields = dict([(field.name, field.dataType)\n",
    "                             for field in df2.schema.fields\n",
    "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
    "   while len(complex_fields)!=0:\n",
    "      col_name=list(complex_fields.keys())[0]\n",
    "      # print (\"Processing :\"+col_name+\" Type : \"+str(type(complex_fields[col_name])))\n",
    "    \n",
    "      # if StructType then convert all sub element to columns.\n",
    "      # i.e. flatten structs\n",
    "      if (type(complex_fields[col_name]) == StructType):\n",
    "         expanded = [col(col_name+'.'+k).alias(col_name+'_'+k) for k in [ n.name for n in  complex_fields[col_name]]]\n",
    "         df2=df2.select(\"*\", *expanded).drop(col_name)\n",
    "    \n",
    "      # if ArrayType then add the Array Elements as Rows using the explode function\n",
    "      # i.e. explode Arrays\n",
    "      elif (type(complex_fields[col_name]) == ArrayType):    \n",
    "         df2=df2.withColumn(col_name,explode_outer(col_name))\n",
    "    \n",
    "      # recompute remaining Complex Fields in Schema       \n",
    "      complex_fields = dict([(field.name, field.dataType)\n",
    "                             for field in df2.schema.fields\n",
    "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
    "   return df2\n",
    "\n",
    "df=flatten(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How often diff event-types occurred in the extracted data? (Using PySparks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('type').count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving Label to User Based on whether they are using through an organisation or not(Using PySparks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.withColumn(\"User_Group\",\n",
    "                     when((df.org_id.isNull()), lit(\"Individual_users\")).otherwise(lit(\"Org_User\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Events distribution for OrgUsers and Individual user (Using PySpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Event_Distribution=df.groupby(['User_Group','type']).count()\n",
    "Event_Distribution.groupby('User_Group').pivot('type').sum('count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the popular Languages on Github, based on extracted data?(Using PySpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('payload_pull_request_base_repo_language').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do org_users trigger more events or individual users? (Using PySpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('User_Group').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programming Languages Used by the User Groups(using PySpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Language_User_Group=df.groupby(['User_Group','payload_pull_request_base_repo_language']).count()\n",
    "Language_User_Group.groupby('payload_pull_request_base_repo_language').pivot('User_Group').sum('count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No of People Working on Multiple Reposistories {Individuals/Organisational}(Using PySpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_repo_actors_PS=df.groupby(['User_Group','actor_id','repo_id']).count().groupby(['User_Group','actor_id']).count()\n",
    "multiple_repo_actors_PS.filter(multiple_repo_actors_PS['count']>1).groupby('User_Group').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average (Commits vs Distinct) Commit per Push Events for User Type (PySpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_commits=df.filter(df['type']=='PushEvent').select(['User_Group','payload_size','payload_distinct_size']).groupby('User_Group').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Organisation to target for GithubPro(Calculated using No of Events) (Using Pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.org_login.isNotNull()).groupby('org_login').count().sort('count', ascending=False).show(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Organisation to target for GithubPro(Calculated using No of Distinct Users)(Using PySparks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.org_login.isNotNull()).select(['org_login', 'actor_id','type']).groupby(['org_login','actor_id']).\\\n",
    "                                    count().select(['org_login', 'actor_id']).groupby('org_login').count()\\\n",
    "                                    .sort('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Number of Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most Starred Repo (PySpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df['type']=='WatchEvent').groupby('repo_name').count().sort('count',ascending=False).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
